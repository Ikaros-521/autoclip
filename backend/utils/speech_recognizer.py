"""
è¯­éŸ³è¯†åˆ«å·¥å…· - æ”¯æŒå¤šç§è¯­éŸ³è¯†åˆ«æœåŠ¡
æ”¯æŒæœ¬åœ°Whisperã€OpenAI APIã€Azure Speech Servicesç­‰å¤šç§è¯­éŸ³è¯†åˆ«æœåŠ¡
"""
import logging
import subprocess
import json
import os
import asyncio
from typing import Optional, List, Dict, Any, Union
from pathlib import Path
from enum import Enum
import requests
from dataclasses import dataclass

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥bcut-asr
try:
    from bcut_asr import BcutASR
    from bcut_asr.orm import ResultStateEnum
    BCUT_ASR_AVAILABLE = True
except ImportError:
    BCUT_ASR_AVAILABLE = False
    logger.warning("bcut-asræœªå®‰è£…ï¼Œå°†è·³è¿‡bcut-asræ–¹æ³•")

def _auto_install_bcut_asr():
    """è‡ªåŠ¨å®‰è£…bcut-asr"""
    try:
        import subprocess
        import sys
        from pathlib import Path
        
        # è·å–å®‰è£…è„šæœ¬è·¯å¾„
        script_path = Path(__file__).parent.parent.parent / "scripts" / "install_bcut_asr.py"
        
        if not script_path.exists():
            logger.error("å®‰è£…è„šæœ¬ä¸å­˜åœ¨ï¼Œè¯·æ‰‹åŠ¨å®‰è£…bcut-asr")
            _show_manual_install_guide()
            return False
        
        logger.info("å¼€å§‹è‡ªåŠ¨å®‰è£…bcut-asr...")
        
        # è¿è¡Œå®‰è£…è„šæœ¬
        result = subprocess.run([
            sys.executable, str(script_path)
        ], capture_output=True, text=True, timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
        
        if result.returncode == 0:
            logger.info("âœ… bcut-asrè‡ªåŠ¨å®‰è£…æˆåŠŸ")
            return True
        else:
            logger.error(f"âŒ bcut-asrè‡ªåŠ¨å®‰è£…å¤±è´¥: {result.stderr}")
            _show_manual_install_guide()
            return False
            
    except subprocess.TimeoutExpired:
        logger.error("âŒ bcut-asrå®‰è£…è¶…æ—¶")
        _show_manual_install_guide()
        return False
    except Exception as e:
        logger.error(f"âŒ bcut-asrè‡ªåŠ¨å®‰è£…å¤±è´¥: {e}")
        _show_manual_install_guide()
        return False

def _show_manual_install_guide():
    """æ˜¾ç¤ºæ‰‹åŠ¨å®‰è£…æŒ‡å¯¼"""
    logger.info("ğŸ“‹ æ‰‹åŠ¨å®‰è£…æŒ‡å¯¼:")
    logger.info("1. å®‰è£… ffmpeg:")
    logger.info("   macOS: brew install ffmpeg")
    logger.info("   Ubuntu: sudo apt install ffmpeg")
    logger.info("   Windows: winget install ffmpeg")
    logger.info("2. å®‰è£… bcut-asr:")
    logger.info("   git clone https://github.com/SocialSisterYi/bcut-asr.git")
    logger.info("   cd bcut-asr && pip install .")
    logger.info("3. è¿è¡Œæ‰‹åŠ¨å®‰è£…è„šæœ¬:")
    logger.info("   python scripts/manual_install_guide.py")

def _ensure_bcut_asr_available():
    """ç¡®ä¿bcut-asrå¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å°è¯•è‡ªåŠ¨å®‰è£…"""
    global BCUT_ASR_AVAILABLE
    
    if BCUT_ASR_AVAILABLE:
        return True
    
    logger.info("bcut-asrä¸å¯ç”¨")
    
    # if _auto_install_bcut_asr():
    #     # é‡æ–°å°è¯•å¯¼å…¥
    #     try:
    #         from bcut_asr import BcutASR
    #         from bcut_asr.orm import ResultStateEnum
    #         BCUT_ASR_AVAILABLE = True
    #         logger.info("âœ… bcut-asrå®‰è£…æˆåŠŸï¼Œç°åœ¨å¯ä»¥ä½¿ç”¨")
    #         return True
    #     except ImportError:
    #         logger.error("âŒ bcut-asrå®‰è£…åä»æ— æ³•å¯¼å…¥")
    #         return False
    # else:
    #     logger.warning("âš ï¸ bcut-asrè‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œå°†ä½¿ç”¨å…¶ä»–æ–¹æ³•")
    #     return False


class SpeechRecognitionMethod(str, Enum):
    """è¯­éŸ³è¯†åˆ«æ–¹æ³•æšä¸¾"""
    BCUT_ASR = "bcut_asr"
    WHISPER_LOCAL = "whisper_local"
    OPENAI_API = "openai_api"
    AZURE_SPEECH = "azure_speech"
    GOOGLE_SPEECH = "google_speech"
    ALIYUN_SPEECH = "aliyun_speech"


class LanguageCode(str, Enum):
    """æ”¯æŒçš„è¯­è¨€ä»£ç """
    # ä¸­æ–‡
    CHINESE_SIMPLIFIED = "zh"
    CHINESE_TRADITIONAL = "zh-TW"
    # è‹±æ–‡
    ENGLISH = "en"
    ENGLISH_US = "en-US"
    ENGLISH_UK = "en-GB"
    # æ—¥æ–‡
    JAPANESE = "ja"
    # éŸ©æ–‡
    KOREAN = "ko"
    # æ³•æ–‡
    FRENCH = "fr"
    # å¾·æ–‡
    GERMAN = "de"
    # è¥¿ç­ç‰™æ–‡
    SPANISH = "es"
    # ä¿„æ–‡
    RUSSIAN = "ru"
    # é˜¿æ‹‰ä¼¯æ–‡
    ARABIC = "ar"
    # è‘¡è„ç‰™æ–‡
    PORTUGUESE = "pt"
    # æ„å¤§åˆ©æ–‡
    ITALIAN = "it"
    # è‡ªåŠ¨æ£€æµ‹
    AUTO = "auto"


@dataclass
class SpeechRecognitionConfig:
    """è¯­éŸ³è¯†åˆ«é…ç½®"""
    method: SpeechRecognitionMethod = SpeechRecognitionMethod.BCUT_ASR
    language: LanguageCode = LanguageCode.AUTO
    model: str = "base"  # Whisperæ¨¡å‹å¤§å°
    timeout: int = 0  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶
    output_format: str = "srt"  # è¾“å‡ºæ ¼å¼
    enable_timestamps: bool = True  # æ˜¯å¦å¯ç”¨æ—¶é—´æˆ³
    enable_punctuation: bool = True  # æ˜¯å¦å¯ç”¨æ ‡ç‚¹ç¬¦å·
    enable_speaker_diarization: bool = False  # æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»
    enable_fallback: bool = True  # æ˜¯å¦å¯ç”¨å›é€€æœºåˆ¶
    fallback_method: SpeechRecognitionMethod = SpeechRecognitionMethod.WHISPER_LOCAL  # å›é€€æ–¹æ³•
    
    # OpenAI API è®¾ç½®
    openai_api_key: Optional[str] = None
    openai_base_url: Optional[str] = None
    
    def __post_init__(self):
        """éªŒè¯é…ç½®å‚æ•°"""
        # éªŒè¯æ–¹æ³•
        if not isinstance(self.method, SpeechRecognitionMethod):
            try:
                self.method = SpeechRecognitionMethod(self.method)
            except ValueError:
                raise ValueError(f"ä¸æ”¯æŒçš„è¯­éŸ³è¯†åˆ«æ–¹æ³•: {self.method}")
        
        # éªŒè¯è¯­è¨€
        if not isinstance(self.language, LanguageCode):
            try:
                self.language = LanguageCode(self.language)
            except ValueError:
                raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€ä»£ç : {self.language}")
        
        # éªŒè¯æ¨¡å‹
        valid_models = ["tiny", "base", "small", "medium", "large"]
        if self.model not in valid_models and self.method == SpeechRecognitionMethod.WHISPER_LOCAL:
            raise ValueError(f"ä¸æ”¯æŒçš„Whisperæ¨¡å‹: {self.model}")
        
        # éªŒè¯è¶…æ—¶æ—¶é—´
        if self.timeout < 0:
            raise ValueError("è¶…æ—¶æ—¶é—´ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        # éªŒè¯è¾“å‡ºæ ¼å¼
        valid_formats = ["srt", "vtt", "txt", "json"]
        if self.output_format not in valid_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {self.output_format}")


class SpeechRecognitionError(Exception):
    """è¯­éŸ³è¯†åˆ«é”™è¯¯"""
    pass


class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨ï¼Œæ”¯æŒå¤šç§è¯­éŸ³è¯†åˆ«æœåŠ¡"""
    
    def __init__(self, config: Optional[SpeechRecognitionConfig] = None):
        self.config = config or SpeechRecognitionConfig()
        self.available_methods = self._check_available_methods()
    
    def _check_available_methods(self) -> Dict[SpeechRecognitionMethod, bool]:
        """æ£€æŸ¥å¯ç”¨çš„è¯­éŸ³è¯†åˆ«æ–¹æ³•"""
        methods = {}
        
        # æ£€æŸ¥bcut-asr
        methods[SpeechRecognitionMethod.BCUT_ASR] = self._check_bcut_asr_availability()
        
        # æ£€æŸ¥æœ¬åœ°Whisper
        methods[SpeechRecognitionMethod.WHISPER_LOCAL] = self._check_whisper_availability()
        
        # æ£€æŸ¥OpenAI API
        methods[SpeechRecognitionMethod.OPENAI_API] = self._check_openai_availability()
        
        # æ£€æŸ¥Azure Speech Services
        methods[SpeechRecognitionMethod.AZURE_SPEECH] = self._check_azure_speech_availability()
        
        # æ£€æŸ¥Google Speech-to-Text
        methods[SpeechRecognitionMethod.GOOGLE_SPEECH] = self._check_google_speech_availability()
        
        # æ£€æŸ¥é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«
        methods[SpeechRecognitionMethod.ALIYUN_SPEECH] = self._check_aliyun_speech_availability()
        
        return methods
    
    def _check_bcut_asr_availability(self) -> bool:
        """æ£€æŸ¥bcut-asræ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™å°è¯•è‡ªåŠ¨å®‰è£…"""
        return BCUT_ASR_AVAILABLE
        if BCUT_ASR_AVAILABLE:
            return True
        
        # å°è¯•è‡ªåŠ¨å®‰è£…
        logger.info("bcut-asrä¸å¯ç”¨ï¼Œå°è¯•è‡ªåŠ¨å®‰è£…...")
        if _ensure_bcut_asr_available():
            return True
        
        logger.warning("bcut-asrä¸å¯ç”¨ä¸”è‡ªåŠ¨å®‰è£…å¤±è´¥")
        return False
    
    def _check_whisper_availability(self) -> bool:
        """æ£€æŸ¥æœ¬åœ°Whisperæ˜¯å¦å¯ç”¨ (æ”¯æŒ faster-whisper å’Œ openai-whisper)"""
        # 1. ä¼˜å…ˆæ£€æŸ¥ faster-whisper (PythonåŒ…)
        try:
            import faster_whisper
            logger.info("æ£€æµ‹åˆ° faster-whisper å·²å®‰è£…")
            return True
        except ImportError:
            pass

        # 2. æ£€æŸ¥ whisper (å‘½ä»¤è¡Œå·¥å…·)
        try:
            result = subprocess.run(['whisper', '--help'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                logger.info("æ£€æµ‹åˆ° openai-whisper CLI å·²å®‰è£…")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
            
        logger.warning("æœ¬åœ°Whisperæœªå®‰è£…æˆ–ä¸å¯ç”¨ (æœªæ£€æµ‹åˆ° faster-whisper æˆ– whisper CLI)")
        return False
    
    def _check_openai_availability(self) -> bool:
        """æ£€æŸ¥OpenAI APIæ˜¯å¦å¯ç”¨ (ä»…æ£€æŸ¥åº“æ˜¯å¦å®‰è£…)"""
        try:
            import openai
            return True
        except ImportError:
            return False
    
    def _check_azure_speech_availability(self) -> bool:
        """æ£€æŸ¥Azure Speech Servicesæ˜¯å¦å¯ç”¨"""
        api_key = os.getenv("AZURE_SPEECH_KEY")
        region = os.getenv("AZURE_SPEECH_REGION")
        return api_key is not None and region is not None
    
    def _check_google_speech_availability(self) -> bool:
        """æ£€æŸ¥Google Speech-to-Textæ˜¯å¦å¯ç”¨"""
        # æ£€æŸ¥Google Cloudå‡­è¯æ–‡ä»¶
        cred_file = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        if cred_file and Path(cred_file).exists():
            return True
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("GOOGLE_SPEECH_API_KEY")
        return api_key is not None
    
    def _check_aliyun_speech_availability(self) -> bool:
        """æ£€æŸ¥é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«æ˜¯å¦å¯ç”¨"""
        access_key = os.getenv("ALIYUN_ACCESS_KEY_ID")
        secret_key = os.getenv("ALIYUN_ACCESS_KEY_SECRET")
        app_key = os.getenv("ALIYUN_SPEECH_APP_KEY")
        return access_key is not None and secret_key is not None and app_key is not None
    
    def _extract_audio_from_video(self, video_path: Path, output_dir: Path, audio_format: str = "wav", bitrate: str = "64k") -> Path:
        """
        ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            audio_format: éŸ³é¢‘æ ¼å¼ (wav, mp3)
            bitrate: æ¯”ç‰¹ç‡ (ä»…mp3æœ‰æ•ˆ)ï¼Œé»˜è®¤64k
            
        Returns:
            æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        try:
            # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
            result = subprocess.run(['ffmpeg', '-version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode != 0:
                raise SpeechRecognitionError("ffmpegä¸å¯ç”¨ï¼Œè¯·å®‰è£…ffmpeg")
            
            # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶è·¯å¾„
            audio_filename = f"{video_path.stem}_audio.{audio_format}"
            audio_path = output_dir / audio_filename
            
            # å¦‚æœéŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if audio_path.exists():
                logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {audio_path}")
                return audio_path
            
            logger.info(f"æ­£åœ¨ä»è§†é¢‘æå–éŸ³é¢‘: {video_path} -> {audio_path}")
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            if audio_format == "mp3":
                cmd = [
                    'ffmpeg',
                    '-i', str(video_path),
                    '-vn',  # ä¸å¤„ç†è§†é¢‘æµ
                    '-acodec', 'libmp3lame',
                    '-b:a', bitrate,
                    '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                    str(audio_path)
                ]
            else:
                # é»˜è®¤ wav (pcm_s16le)
                cmd = [
                    'ffmpeg',
                    '-i', str(video_path),
                    '-vn',  # ä¸å¤„ç†è§†é¢‘æµ
                    '-acodec', 'pcm_s16le',  # ä½¿ç”¨PCM 16ä½ç¼–ç 
                    '-ar', '16000',  # é‡‡æ ·ç‡16kHz
                    '-ac', '1',  # å•å£°é“
                    '-y',  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                    str(audio_path)
                ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                # å¦‚æœmp3ç¼–ç å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ²¡æœ‰libmp3lameï¼‰ï¼Œå°è¯•ä½¿ç”¨aac
                if audio_format == "mp3" and "Encoder (codec libmp3lame) not found" in result.stderr:
                     logger.warning("æœªæ‰¾åˆ°libmp3lameç¼–ç å™¨ï¼Œå°è¯•ä½¿ç”¨aac...")
                     audio_path_aac = audio_path.with_suffix(".m4a")
                     cmd = [
                        'ffmpeg',
                        '-i', str(video_path),
                        '-vn',
                        '-acodec', 'aac',
                        '-b:a', bitrate,
                        '-y',
                        str(audio_path_aac)
                     ]
                     result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                     if result.returncode == 0:
                         return audio_path_aac
                
                raise SpeechRecognitionError(f"éŸ³é¢‘æå–å¤±è´¥: {result.stderr}")
            
            if not audio_path.exists():
                raise SpeechRecognitionError("éŸ³é¢‘æå–å¤±è´¥ï¼Œè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
            
            logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
            return audio_path
            
        except subprocess.TimeoutExpired:
            raise SpeechRecognitionError("éŸ³é¢‘æå–è¶…æ—¶")
        except Exception as e:
            raise SpeechRecognitionError(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
    
    def _split_audio_file(self, audio_path: Path, segment_duration: int) -> List[Path]:
        """
        ä½¿ç”¨ffmpegåˆ‡åˆ†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            segment_duration: åˆ‡åˆ†æ—¶é•¿ï¼ˆç§’ï¼‰
            
        Returns:
            åˆ‡åˆ†åçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            output_pattern = str(audio_path.parent / f"{audio_path.stem}_%03d{audio_path.suffix}")
            
            cmd = [
                'ffmpeg',
                '-i', str(audio_path),
                '-f', 'segment',
                '-segment_time', str(segment_duration),
                '-c', 'copy',
                '-y',
                output_pattern
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode != 0:
                raise SpeechRecognitionError(f"éŸ³é¢‘åˆ‡åˆ†å¤±è´¥: {result.stderr}")
                
            # è·å–ç”Ÿæˆçš„ç‰‡æ®µæ–‡ä»¶
            segment_files = sorted(list(audio_path.parent.glob(f"{audio_path.stem}_*{audio_path.suffix}")))
            # æ’é™¤åŸå§‹æ–‡ä»¶
            segment_files = [f for f in segment_files if f.name != audio_path.name]
            
            return segment_files
            
        except Exception as e:
            raise SpeechRecognitionError(f"éŸ³é¢‘åˆ‡åˆ†å¤±è´¥: {e}")

    def _parse_srt_timestamp(self, timestamp: str) -> float:
        """è§£æSRTæ—¶é—´æˆ³ä¸ºç§’æ•°"""
        # 00:00:00,000
        try:
            time_parts = timestamp.replace(',', '.').split(':')
            hours = int(time_parts[0])
            minutes = int(time_parts[1])
            seconds = float(time_parts[2])
            return hours * 3600 + minutes * 60 + seconds
        except Exception:
            return 0.0

    def _adjust_srt_content(self, srt_content: str, time_offset: float, start_index: int) -> tuple[str, int]:
        """
        è°ƒæ•´SRTå†…å®¹çš„æ—¶é—´æˆ³å’Œåºå·
        
        Args:
            srt_content: SRTå†…å®¹
            time_offset: æ—¶é—´åç§»é‡ï¼ˆç§’ï¼‰
            start_index: èµ·å§‹åºå·
            
        Returns:
            (è°ƒæ•´åçš„SRTå†…å®¹, ä¸‹ä¸€ä¸ªåºå·)
        """
        lines = srt_content.strip().split('\n')
        adjusted_lines = []
        current_index = start_index
        
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line:
                i += 1
                continue
                
            # å°è¯•è§£æåºå·
            if line.isdigit():
                # å†™å…¥æ–°åºå·
                adjusted_lines.append(str(current_index))
                current_index += 1
                
                # ä¸‹ä¸€è¡Œåº”è¯¥æ˜¯æ—¶é—´æˆ³
                if i + 1 < len(lines):
                    time_line = lines[i+1].strip()
                    if '-->' in time_line:
                        try:
                            start_str, end_str = time_line.split(' --> ')
                            start_seconds = self._parse_srt_timestamp(start_str) + time_offset
                            end_seconds = self._parse_srt_timestamp(end_str) + time_offset
                            
                            new_time_line = f"{self._format_timestamp(start_seconds)} --> {self._format_timestamp(end_seconds)}"
                            adjusted_lines.append(new_time_line)
                        except Exception:
                             adjusted_lines.append(time_line)
                        i += 2
                    else:
                        # æ ¼å¼ä¸å¯¹ï¼Œç›´æ¥å¤åˆ¶
                        adjusted_lines.append(lines[i+1])
                        i += 2
                else:
                    i += 1
                
                # åé¢çš„è¡Œæ˜¯å­—å¹•å†…å®¹ï¼Œç›´åˆ°é‡åˆ°ç©ºè¡Œæˆ–ä¸‹ä¸€ä¸ªæ•°å­—
                while i < len(lines):
                    content_line = lines[i].strip()
                    if not content_line:
                        adjusted_lines.append("")
                        i += 1
                        break
                    # Check if it looks like start of new block
                    if content_line.isdigit() and i+1 < len(lines) and '-->' in lines[i+1]:
                         break
                    
                    adjusted_lines.append(lines[i])
                    i += 1
            else:
                # ä¸æ˜¯æ•°å­—å¼€å¤´ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶å¤´çš„å…ƒæ•°æ®æˆ–å…¶ä»–ï¼Œç›´æ¥å¤åˆ¶
                adjusted_lines.append(line)
                i += 1
                
        return "\n".join(adjusted_lines), current_index

    def _json_to_srt(self, json_data: Dict[str, Any]) -> str:
        """å°†OpenAI JSONå“åº”è½¬æ¢ä¸ºSRTæ ¼å¼"""
        if "segments" not in json_data:
            return ""
        
        segments = json_data["segments"]
        srt_parts = []
        
        for i, segment in enumerate(segments, start=1):
            start = self._format_timestamp(segment.get("start", 0))
            end = self._format_timestamp(segment.get("end", 0))
            text = segment.get("text", "").strip()
            
            srt_parts.append(f"{i}\n{start} --> {end}\n{text}\n")
            
        return "\n".join(srt_parts)

    def generate_subtitle(self, video_path: Path, output_path: Optional[Path] = None, 
                         config: Optional[SpeechRecognitionConfig] = None) -> Path:
        """
        ç”Ÿæˆå­—å¹•æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„
            config: è¯­éŸ³è¯†åˆ«é…ç½®
            
        Returns:
            ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶è·¯å¾„
            
        Raises:
            SpeechRecognitionError: è¯­éŸ³è¯†åˆ«å¤±è´¥
        """
        if not video_path.exists():
            raise SpeechRecognitionError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # ä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–é»˜è®¤é…ç½®
        config = config or self.config
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_path is None:
            output_path = video_path.parent / f"{video_path.stem}.{config.output_format}"
        
        # æ ¹æ®é…ç½®çš„æ–¹æ³•é€‰æ‹©è¯†åˆ«æœåŠ¡ï¼Œæ”¯æŒå›é€€æœºåˆ¶
        try:
            if config.method == SpeechRecognitionMethod.BCUT_ASR:
                return self._generate_subtitle_bcut_asr(video_path, output_path, config)
            elif config.method == SpeechRecognitionMethod.WHISPER_LOCAL:
                return self._generate_subtitle_whisper_local(video_path, output_path, config)
            elif config.method == SpeechRecognitionMethod.OPENAI_API:
                return self._generate_subtitle_openai_api(video_path, output_path, config)
            elif config.method == SpeechRecognitionMethod.AZURE_SPEECH:
                return self._generate_subtitle_azure_speech(video_path, output_path, config)
            elif config.method == SpeechRecognitionMethod.GOOGLE_SPEECH:
                return self._generate_subtitle_google_speech(video_path, output_path, config)
            elif config.method == SpeechRecognitionMethod.ALIYUN_SPEECH:
                return self._generate_subtitle_aliyun_speech(video_path, output_path, config)
            else:
                raise SpeechRecognitionError(f"ä¸æ”¯æŒçš„è¯­éŸ³è¯†åˆ«æ–¹æ³•: {config.method}")
        except SpeechRecognitionError as e:
            # å¦‚æœå¯ç”¨äº†å›é€€æœºåˆ¶ä¸”å½“å‰æ–¹æ³•ä¸æ˜¯å›é€€æ–¹æ³•ï¼Œåˆ™å°è¯•å›é€€
            if (config.enable_fallback and 
                config.method != config.fallback_method and 
                self.available_methods.get(config.fallback_method, False)):
                
                logger.warning(f"ä¸»æ–¹æ³• {config.method} å¤±è´¥: {e}")
                logger.info(f"å°è¯•å›é€€åˆ° {config.fallback_method}")
                
                # åˆ›å»ºå›é€€é…ç½®
                fallback_config = SpeechRecognitionConfig(
                    method=config.fallback_method,
                    language=config.language,
                    model=config.model,
                    timeout=config.timeout,
                    output_format=config.output_format,
                    enable_timestamps=config.enable_timestamps,
                    enable_punctuation=config.enable_punctuation,
                    enable_speaker_diarization=config.enable_speaker_diarization,
                    enable_fallback=False  # é¿å…æ— é™å›é€€
                )
                
                return self.generate_subtitle(video_path, output_path, fallback_config)
            else:
                raise
    
    def _generate_subtitle_bcut_asr(self, video_path: Path, output_path: Path, 
                                   config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨bcut-asrç”Ÿæˆå­—å¹•"""
        # ç¡®ä¿bcut-asrå¯ç”¨
        if not _ensure_bcut_asr_available():
            raise SpeechRecognitionError(
                "bcut-asrä¸å¯ç”¨ä¸”è‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…:\n"
                "1. è¿è¡Œ: python scripts/install_bcut_asr.py\n"
                "2. æˆ–æ‰‹åŠ¨å®‰è£…: git clone https://github.com/SocialSisterYi/bcut-asr.git\n"
                "3. åŒæ—¶ç¡®ä¿å·²å®‰è£…ffmpeg:\n"
                "   macOS: brew install ffmpeg\n"
                "   Ubuntu: sudo apt install ffmpeg\n"
                "   Windows: winget install ffmpeg"
            )
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨bcut-asrç”Ÿæˆå­—å¹•: {video_path}")
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not video_path.exists():
                raise SpeechRecognitionError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size = video_path.stat().st_size
            if file_size == 0:
                raise SpeechRecognitionError(f"è§†é¢‘æ–‡ä»¶ä¸ºç©º: {video_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼ï¼Œå¦‚æœæ˜¯è§†é¢‘æ–‡ä»¶éœ€è¦å…ˆæå–éŸ³é¢‘
            audio_path = self._extract_audio_from_video(video_path, output_path.parent)
            
            # åˆ›å»ºBcutASRå®ä¾‹ï¼Œä½¿ç”¨éŸ³é¢‘æ–‡ä»¶
            asr = BcutASR(str(audio_path))
            
            # ä¸Šä¼ æ–‡ä»¶
            logger.info("æ­£åœ¨ä¸Šä¼ æ–‡ä»¶åˆ°bcut-asr...")
            asr.upload()
            
            # åˆ›å»ºä»»åŠ¡
            logger.info("æ­£åœ¨åˆ›å»ºè¯†åˆ«ä»»åŠ¡...")
            asr.create_task()
            
            # è½®è¯¢æ£€æŸ¥ç»“æœ
            logger.info("æ­£åœ¨ç­‰å¾…è¯†åˆ«ç»“æœ...")
            max_attempts = 60  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿï¼ˆæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
            attempt = 0
            
            while attempt < max_attempts:
                result = asr.result()
                
                # åˆ¤æ–­è¯†åˆ«æˆåŠŸ
                if result.state == ResultStateEnum.COMPLETE:
                    logger.info("bcut-asrè¯†åˆ«å®Œæˆ")
                    break
                elif result.state == ResultStateEnum.FAILED:
                    raise SpeechRecognitionError("bcut-asrè¯†åˆ«å¤±è´¥")
                
                # ç­‰å¾…5ç§’åé‡è¯•
                import time
                time.sleep(5)
                attempt += 1
                logger.info(f"ç­‰å¾…è¯†åˆ«ç»“æœ... ({attempt}/{max_attempts})")
            else:
                raise SpeechRecognitionError("bcut-asrè¯†åˆ«è¶…æ—¶")
            
            # è§£æå­—å¹•å†…å®¹
            subtitle = result.parse()
            
            # åˆ¤æ–­æ˜¯å¦å­˜åœ¨å­—å¹•
            if not subtitle.has_data():
                raise SpeechRecognitionError("bcut-asræœªè¯†åˆ«åˆ°æœ‰æ•ˆå­—å¹•å†…å®¹")
            
            # æ ¹æ®è¾“å‡ºæ ¼å¼ä¿å­˜å­—å¹•
            if config.output_format == "srt":
                subtitle_content = subtitle.to_srt()
            elif config.output_format == "json":
                subtitle_content = subtitle.to_json()
            elif config.output_format == "lrc":
                subtitle_content = subtitle.to_lrc()
            elif config.output_format == "txt":
                subtitle_content = subtitle.to_txt()
            else:
                # é»˜è®¤ä½¿ç”¨srtæ ¼å¼
                subtitle_content = subtitle.to_srt()
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(subtitle_content)
                
            # å¦‚æœè¾“å‡ºæ ¼å¼ä¸æ˜¯SRTï¼Œé¢å¤–ä¿å­˜ä¸€ä»½SRTæ–‡ä»¶
            if config.output_format != "srt":
                srt_path = output_path.with_suffix('.srt')
                try:
                    with open(srt_path, 'w', encoding='utf-8') as f:
                        f.write(subtitle.to_srt())
                    logger.info(f"é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {srt_path}")
                except Exception as e:
                    logger.warning(f"æ— æ³•é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {e}")
            
            logger.info(f"bcut-asrå­—å¹•ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
            
        except Exception as e:
            error_msg = f"bcut-asrç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}\n"
            error_msg += "å¯èƒ½çš„åŸå› :\n"
            error_msg += "1. ç½‘ç»œè¿æ¥é—®é¢˜\n"
            error_msg += "2. æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ\n"
            error_msg += "3. æ–‡ä»¶è¿‡å¤§\n"
            error_msg += "4. bcut-asræœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def _format_timestamp(self, seconds: float) -> str:
        """è½¬æ¢ç§’æ•°ä¸ºSRTæ—¶é—´æˆ³æ ¼å¼ (HH:MM:SS,mmm)"""
        whole_seconds = int(seconds)
        milliseconds = int((seconds - whole_seconds) * 1000)
        
        hours = whole_seconds // 3600
        minutes = (whole_seconds % 3600) // 60
        seconds = whole_seconds % 60
        
        return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

    def _generate_subtitle_faster_whisper(self, video_path: Path, output_path: Path, 
                                        config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨ faster-whisper ç”Ÿæˆå­—å¹•"""
        try:
            from faster_whisper import WhisperModel
            import torch
        except ImportError:
            raise SpeechRecognitionError("faster-whisper æœªå®‰è£…")

        logger.info(f"å¼€å§‹ä½¿ç”¨ faster-whisper ç”Ÿæˆå­—å¹•: {video_path}")
        
        try:
            # ç¡®å®šè¿è¡Œè®¾å¤‡
            device = "cuda" if torch.cuda.is_available() else "cpu"
            compute_type = "float16" if device == "cuda" else "int8"
            logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}, è®¡ç®—ç±»å‹: {compute_type}")
            
            # åŠ è½½æ¨¡å‹
            model_size = config.model
            model = WhisperModel(model_size, device=device, compute_type=compute_type)
            
            # è½¬å½•
            segments, info = model.transcribe(
                str(video_path), 
                beam_size=5,
                language=None if config.language == LanguageCode.AUTO else config.language,
                vad_filter=True  # å¯ç”¨VADè¿‡æ»¤é™éŸ³
            )
            
            logger.info(f"æ£€æµ‹åˆ°è¯­è¨€: {info.language}, æ¦‚ç‡: {info.language_probability:.2f}")
            
            # ç”ŸæˆSRTå†…å®¹
            srt_lines = []
            for i, segment in enumerate(segments, start=1):
                start_time = self._format_timestamp(segment.start)
                end_time = self._format_timestamp(segment.end)
                text = segment.text.strip()
                
                srt_lines.append(f"{i}")
                srt_lines.append(f"{start_time} --> {end_time}")
                srt_lines.append(f"{text}\n")
            
            # å†™å…¥æ–‡ä»¶
            srt_content = "\n".join(srt_lines)
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(srt_content)
                
            # å¦‚æœè¾“å‡ºè·¯å¾„ä¸æ˜¯.srtç»“å°¾ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½.srt
            if output_path.suffix.lower() != ".srt":
                srt_path = output_path.with_suffix('.srt')
                try:
                    with open(srt_path, "w", encoding="utf-8") as f:
                        f.write(srt_content)
                    logger.info(f"é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {srt_path}")
                except Exception as e:
                    logger.warning(f"æ— æ³•é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {e}")
                
            logger.info(f"faster-whisper å­—å¹•ç”ŸæˆæˆåŠŸ: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"faster-whisper æ‰§è¡Œå¤±è´¥: {e}")
            raise SpeechRecognitionError(f"faster-whisper æ‰§è¡Œå¤±è´¥: {e}")

    def _generate_subtitle_whisper_local(self, video_path: Path, output_path: Path, 
                                       config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨æœ¬åœ°Whisperç”Ÿæˆå­—å¹• (ä¼˜å…ˆä½¿ç”¨ faster-whisper)"""
        # å°è¯•ä½¿ç”¨ faster-whisper
        try:
            import faster_whisper
            return self._generate_subtitle_faster_whisper(video_path, output_path, config)
        except ImportError:
            pass

        # Fallback åˆ°å‘½ä»¤è¡Œ whisper
        if not self.available_methods[SpeechRecognitionMethod.WHISPER_LOCAL]:
            raise SpeechRecognitionError(
                "æœ¬åœ°Whisperä¸å¯ç”¨ï¼Œè¯·å®‰è£… faster-whisper æˆ– openai-whisper:\n"
                "pip install faster-whisper\n"
                "æˆ–\n"
                "pip install openai-whisper"
            )
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨æœ¬åœ°Whisper CLIç”Ÿæˆå­—å¹•: {video_path}")
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not video_path.exists():
                raise SpeechRecognitionError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å¤§å°
            file_size = video_path.stat().st_size
            if file_size == 0:
                raise SpeechRecognitionError(f"è§†é¢‘æ–‡ä»¶ä¸ºç©º: {video_path}")
            
            # æ„å»ºwhisperå‘½ä»¤
            cmd = [
                'whisper',
                str(video_path),
                '--output_dir', str(output_path.parent),
                '--output_format', config.output_format,
                '--model', config.model
            ]
            
            # æ·»åŠ è¯­è¨€å‚æ•°
            if config.language != LanguageCode.AUTO:
                cmd.extend(['--language', config.language])
            
            # æ·»åŠ è¶…æ—¶å¤„ç†
            logger.info(f"æ‰§è¡ŒWhisperå‘½ä»¤: {' '.join(cmd)}")
            
            # æ ¹æ®è¶…æ—¶é…ç½®å†³å®šæ˜¯å¦è®¾ç½®è¶…æ—¶
            if config.timeout > 0:
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    timeout=config.timeout,
                    cwd=str(video_path.parent)  # è®¾ç½®å·¥ä½œç›®å½•
                )
            else:
                # æ— è¶…æ—¶é™åˆ¶
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True, 
                    cwd=str(video_path.parent)  # è®¾ç½®å·¥ä½œç›®å½•
                )
            
            if result.returncode == 0:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if output_path.exists():
                    logger.info(f"æœ¬åœ°Whisperå­—å¹•ç”ŸæˆæˆåŠŸ: {output_path}")
                    
                    # å¦‚æœè¾“å‡ºä¸æ˜¯SRTï¼Œå°è¯•è½¬æ¢
                    if output_path.suffix.lower() != ".srt":
                        try:
                            srt_path = output_path.with_suffix('.srt')
                            if not srt_path.exists():
                                if output_path.suffix.lower() == ".json":
                                    # å°è¯•ä»JSONè½¬æ¢
                                    import json
                                    with open(output_path, 'r', encoding='utf-8') as f:
                                        data = json.load(f)
                                    
                                    # å°è¯•è½¬æ¢
                                    srt_content = self._json_to_srt(data)
                                    if srt_content:
                                        with open(srt_path, 'w', encoding='utf-8') as f:
                                            f.write(srt_content)
                                        logger.info(f"å·²ä»JSONè½¬æ¢å¹¶ä¿å­˜SRTæ–‡ä»¶: {srt_path}")
                        except Exception as e:
                            logger.warning(f"å°è¯•è½¬æ¢SRTå¤±è´¥: {e}")
                            
                    return output_path
                else:
                    # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„è¾“å‡ºæ–‡ä»¶
                    possible_outputs = list(output_path.parent.glob(f"{video_path.stem}*.{config.output_format}"))
                    if possible_outputs:
                        actual_output = possible_outputs[0]
                        logger.info(f"æ‰¾åˆ°Whisperè¾“å‡ºæ–‡ä»¶: {actual_output}")
                        return actual_output
                    else:
                        raise SpeechRecognitionError(f"Whisperæ‰§è¡ŒæˆåŠŸä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶: {output_path}")
            else:
                error_msg = f"æœ¬åœ°Whisperæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode}):\n"
                if result.stderr:
                    error_msg += f"é”™è¯¯ä¿¡æ¯: {result.stderr}\n"
                if result.stdout:
                    error_msg += f"è¾“å‡ºä¿¡æ¯: {result.stdout}"
                
                # æä¾›å…·ä½“çš„é”™è¯¯è§£å†³å»ºè®®
                if "command not found" in result.stderr:
                    error_msg += "\n\nè§£å†³æ–¹æ¡ˆ: è¯·å®‰è£…whisper: pip install openai-whisper"
                elif "ffmpeg" in result.stderr.lower():
                    error_msg += "\n\nè§£å†³æ–¹æ¡ˆ: è¯·å®‰è£…ffmpeg:\n  macOS: brew install ffmpeg\n  Ubuntu: sudo apt install ffmpeg"
                elif "timeout" in result.stderr.lower():
                    error_msg += f"\n\nè§£å†³æ–¹æ¡ˆ: è§†é¢‘å¤„ç†è¶…æ—¶ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (--model tiny) æˆ–å¢åŠ è¶…æ—¶æ—¶é—´"
                
                logger.error(error_msg)
                raise SpeechRecognitionError(error_msg)
                
        except subprocess.TimeoutExpired:
            error_msg = f"æœ¬åœ°Whisperæ‰§è¡Œè¶…æ—¶ï¼ˆ{config.timeout}ç§’ï¼‰\n"
            error_msg += "è§£å†³æ–¹æ¡ˆ:\n"
            error_msg += "1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹: --model tiny\n"
            error_msg += "2. å¢åŠ è¶…æ—¶æ—¶é—´\n"
            error_msg += "3. æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æŸå"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
        except FileNotFoundError:
            error_msg = "æ‰¾ä¸åˆ°whisperå‘½ä»¤\n"
            error_msg += "è§£å†³æ–¹æ¡ˆ:\n"
            error_msg += "1. å®‰è£…whisper: pip install openai-whisper\n"
            error_msg += "2. ç¡®ä¿whisperåœ¨PATHä¸­: which whisper\n"
            error_msg += "3. é‡æ–°å®‰è£…: pip uninstall openai-whisper && pip install openai-whisper"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
        except Exception as e:
            error_msg = f"æœ¬åœ°Whisperç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}\n"
            error_msg += "è¯·æ£€æŸ¥:\n"
            error_msg += "1. è§†é¢‘æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ\n"
            error_msg += "2. ç³»ç»Ÿæ˜¯å¦æœ‰è¶³å¤Ÿçš„å†…å­˜\n"
            error_msg += "3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def _generate_subtitle_openai_api(self, video_path: Path, output_path: Path, 
                                    config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆå­—å¹•"""
        if not self.available_methods[SpeechRecognitionMethod.OPENAI_API]:
            raise SpeechRecognitionError("OpenAIåº“æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ: pip install openai")
        
        api_key = config.openai_api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
             raise SpeechRecognitionError("æœªé…ç½®OpenAI API Keyï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®æˆ–è®¾ç½®ç¯å¢ƒå˜é‡OPENAI_API_KEY")

        base_url = config.openai_base_url or os.getenv("OPENAI_BASE_URL")
        
        try:
            import openai
            logger.info(f"å¼€å§‹ä½¿ç”¨OpenAI APIç”Ÿæˆå­—å¹•: {video_path}")
            
            # æå–éŸ³é¢‘ (ä½¿ç”¨mp3æ ¼å¼ä»¥èŠ‚çœç©ºé—´ï¼Œ48kbps)
            # OpenAI é™åˆ¶ 25MB
            # 48kbps ä¸‹ï¼Œ25MB å¤§çº¦å¯ä»¥å­˜å‚¨ 72 åˆ†é’Ÿçš„éŸ³é¢‘
            # è®¾ç½®åˆ‡ç‰‡æ—¶é—´ä¸º 20 åˆ†é’Ÿ (1200ç§’)
            audio_path = self._extract_audio_from_video(video_path, output_path.parent, audio_format="mp3", bitrate="48k")
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶å¤§å°ï¼ˆOpenAIé™åˆ¶25MBï¼‰
            file_size = audio_path.stat().st_size
            max_size = 25 * 1024 * 1024
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯
            client_kwargs = {"api_key": api_key}
            if base_url:
                client_kwargs["base_url"] = base_url
            
            client = openai.OpenAI(**client_kwargs)
            
            transcript = ""
            
            # ç¡®å®šæ¨¡å‹åç§°
            model_name = "whisper-1"
            # å¦‚æœconfig.modelä¸æ˜¯æœ¬åœ°æ¨¡å‹çš„æ ‡å‡†åç§°ï¼Œä¸”ä¸ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·é…ç½®çš„
            local_models = ["tiny", "base", "small", "medium", "large", "turbo"]
            if config.model and config.model not in local_models:
                 model_name = config.model
            
            # ç¡®å®šresponse_format
            response_format = "srt"
            if config.output_format == "vtt":
                response_format = "vtt"
            elif config.output_format == "json":
                response_format = "json"
            elif config.output_format == "txt":
                response_format = "text"
            
            if file_size <= max_size:
                # æ–‡ä»¶å°äº25MBï¼Œç›´æ¥å¤„ç†
                logger.info(f"éŸ³é¢‘æ–‡ä»¶å¤§å° ({file_size / 1024 / 1024:.2f}MB) æœªè¶…è¿‡25MBï¼Œç›´æ¥å¤„ç†")
                with open(audio_path, "rb") as audio_file:
                    transcript = client.audio.transcriptions.create(
                        model=model_name, 
                        file=audio_file,
                        response_format=response_format,
                        language=None if config.language == LanguageCode.AUTO else config.language.value
                    )
                    
                    # æ£€æŸ¥æ˜¯å¦è¿”å›äº†JSONä½†æˆ‘ä»¬æƒ³è¦SRT
                    if response_format == "srt":
                        content_str = str(transcript)
                        # logger.warning(content_str)
                        if content_str.strip().startswith("{") and "segments" in content_str:
                            try:
                                import json
                                data = json.loads(content_str)
                                if "segments" in data:
                                    logger.warning("OpenAI APIè¿”å›äº†JSONæ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢ä¸ºSRT...")
                                    transcript = self._json_to_srt(data)
                            except Exception as e:
                                logger.warning(f"å°è¯•è½¬æ¢JSONä¸ºSRTå¤±è´¥: {e}")
            else:
                # æ–‡ä»¶å¤§äº25MBï¼Œéœ€è¦åˆ‡åˆ†
                logger.warning(f"éŸ³é¢‘æ–‡ä»¶å¤§å° ({file_size / 1024 / 1024:.2f}MB) è¶…è¿‡25MBé™åˆ¶ï¼Œå¼€å§‹åˆ‡åˆ†...")
                
                # åˆ‡åˆ†éŸ³é¢‘ (æ¯20åˆ†é’Ÿä¸€æ®µ)
                segment_duration = 1200
                segment_files = self._split_audio_file(audio_path, segment_duration)
                
                logger.info(f"éŸ³é¢‘å·²åˆ‡åˆ†ä¸º {len(segment_files)} ä¸ªç‰‡æ®µï¼Œå¼€å§‹é€ä¸ªè¯†åˆ«...")
                
                full_transcript = ""
                current_index_offset = 1
                
                for i, segment_file in enumerate(segment_files):
                    logger.info(f"æ­£åœ¨è¯†åˆ«ç‰‡æ®µ {i+1}/{len(segment_files)}: {segment_file.name}")
                    
                    try:
                        with open(segment_file, "rb") as audio_file:
                            segment_transcript = client.audio.transcriptions.create(
                                    model=model_name, 
                                    file=audio_file,
                                    response_format=response_format,
                                    language=None if config.language == LanguageCode.AUTO else config.language.value
                                )
                            
                            # æ£€æŸ¥æ˜¯å¦è¿”å›äº†JSONä½†æˆ‘ä»¬æƒ³è¦SRT
                            if response_format == "srt":
                                content_str = str(segment_transcript)
                                # logger.warning(content_str)
                                if content_str.strip().startswith("{") and "segments" in content_str:
                                    try:
                                        import json
                                        data = json.loads(content_str)
                                        
                                        if "segments" in data:
                                            logger.warning(f"OpenAI APIç‰‡æ®µ {i+1} è¿”å›äº†JSONæ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢ä¸ºSRT...")
                                            segment_transcript = self._json_to_srt(data)
                                    except Exception:
                                        pass

                            # åªæœ‰SRTæ ¼å¼æ‰æ”¯æŒåˆå¹¶å’Œè°ƒæ•´æ—¶é—´æˆ³
                            if response_format == "srt":
                                # åˆå¹¶ç»“æœ
                                time_offset = i * segment_duration
                                adjusted_srt, next_index = self._adjust_srt_content(segment_transcript, time_offset, current_index_offset)
                                full_transcript += adjusted_srt + "\n\n"
                                current_index_offset = next_index
                            else:
                                # å…¶ä»–æ ¼å¼ç›´æ¥æ‹¼æ¥ï¼ˆå¯èƒ½ä¸å®Œç¾ï¼Œä½†æš‚ä¸æ”¯æŒå¤æ‚åˆå¹¶ï¼‰
                                full_transcript += str(segment_transcript) + "\n"
                        
                    except Exception as e:
                        logger.error(f"è¯†åˆ«ç‰‡æ®µ {segment_file.name} å¤±è´¥: {e}")
                        raise
                    finally:
                        # æ¸…ç†ç‰‡æ®µæ–‡ä»¶
                        try:
                            if segment_file.exists():
                                os.remove(segment_file)
                        except Exception as e:
                            logger.warning(f"æ— æ³•åˆ é™¤ä¸´æ—¶ç‰‡æ®µæ–‡ä»¶ {segment_file}: {e}")
                
                transcript = full_transcript.strip()
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(str(transcript))
                
            logger.info(f"OpenAI APIå­—å¹•ç”ŸæˆæˆåŠŸ: {output_path}")

            # å¦‚æœè¾“å‡ºæ ¼å¼ä¸æ˜¯SRTï¼Œé¢å¤–ä¿å­˜ä¸€ä»½SRTæ–‡ä»¶
            if config.output_format != "srt":
                srt_path = output_path.with_suffix('.srt')
                try:
                    srt_content = None
                    if response_format == "json":
                        try:
                            # å°è¯•è§£æJSONå†…å®¹
                            import json
                            data = json.loads(str(transcript))
                            # åªæœ‰åŒ…å«segmentsçš„JSONæ‰èƒ½è½¬æ¢ä¸ºSRT
                            if "segments" in data:
                                srt_content = self._json_to_srt(data)
                            else:
                                logger.warning("OpenAI APIè¿”å›çš„JSONä¸åŒ…å«segmentsï¼Œæ— æ³•è½¬æ¢ä¸ºSRT")
                        except Exception:
                            pass
                    elif response_format == "vtt":
                        # å°è¯•å°†VTTå†…å®¹è½¬æ¢ä¸ºSRT
                        content = str(transcript)
                        # ç®€å•çš„VTTè½¬SRTå®ç°
                        lines = content.splitlines()
                        srt_lines = []
                        counter = 1
                        is_header = True
                        for line in lines:
                            if is_header:
                                if line.strip() == "WEBVTT":
                                    continue
                                if line.strip() == "":
                                    is_header = False
                                continue
                            
                            if "-->" in line:
                                srt_lines.append(str(counter))
                                srt_lines.append(line.replace(".", ","))
                                counter += 1
                            else:
                                srt_lines.append(line)
                        
                        srt_content = "\n".join(srt_lines)
                    
                    if srt_content:
                        with open(srt_path, 'w', encoding='utf-8') as f:
                            f.write(srt_content)
                        logger.info(f"é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {srt_path}")
                except Exception as e:
                    logger.warning(f"æ— æ³•é¢å¤–ä¿å­˜SRTæ–‡ä»¶: {e}")

            return output_path
            
        except Exception as e:
            error_msg = f"OpenAI APIç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def _generate_subtitle_azure_speech(self, video_path: Path, output_path: Path, 
                                      config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨Azure Speech Servicesç”Ÿæˆå­—å¹•"""
        if not self.available_methods[SpeechRecognitionMethod.AZURE_SPEECH]:
            raise SpeechRecognitionError("Azure Speech Servicesä¸å¯ç”¨ï¼Œè¯·è®¾ç½®AZURE_SPEECH_KEYå’ŒAZURE_SPEECH_REGIONç¯å¢ƒå˜é‡")
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨Azure Speech Servicesç”Ÿæˆå­—å¹•: {video_path}")
            
            # è¿™é‡Œéœ€è¦å®ç°Azure Speech Servicesè°ƒç”¨
            raise SpeechRecognitionError("Azure Speech ServicesåŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨æœ¬åœ°Whisper")
            
        except Exception as e:
            error_msg = f"Azure Speech Servicesç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def _generate_subtitle_google_speech(self, video_path: Path, output_path: Path, 
                                       config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨Google Speech-to-Textç”Ÿæˆå­—å¹•"""
        if not self.available_methods[SpeechRecognitionMethod.GOOGLE_SPEECH]:
            raise SpeechRecognitionError("Google Speech-to-Textä¸å¯ç”¨ï¼Œè¯·è®¾ç½®GOOGLE_APPLICATION_CREDENTIALSæˆ–GOOGLE_SPEECH_API_KEYç¯å¢ƒå˜é‡")
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨Google Speech-to-Textç”Ÿæˆå­—å¹•: {video_path}")
            
            # è¿™é‡Œéœ€è¦å®ç°Google Speech-to-Textè°ƒç”¨
            raise SpeechRecognitionError("Google Speech-to-TextåŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨æœ¬åœ°Whisper")
            
        except Exception as e:
            error_msg = f"Google Speech-to-Textç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def _generate_subtitle_aliyun_speech(self, video_path: Path, output_path: Path, 
                                       config: SpeechRecognitionConfig) -> Path:
        """ä½¿ç”¨é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«ç”Ÿæˆå­—å¹•"""
        if not self.available_methods[SpeechRecognitionMethod.ALIYUN_SPEECH]:
            raise SpeechRecognitionError("é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨ï¼Œè¯·è®¾ç½®ALIYUN_ACCESS_KEY_IDã€ALIYUN_ACCESS_KEY_SECRETå’ŒALIYUN_SPEECH_APP_KEYç¯å¢ƒå˜é‡")
        
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«ç”Ÿæˆå­—å¹•: {video_path}")
            
            # è¿™é‡Œéœ€è¦å®ç°é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«è°ƒç”¨
            raise SpeechRecognitionError("é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨æœ¬åœ°Whisper")
            
        except Exception as e:
            error_msg = f"é˜¿é‡Œäº‘è¯­éŸ³è¯†åˆ«ç”Ÿæˆå­—å¹•æ—¶å‘ç”Ÿé”™è¯¯: {e}"
            logger.error(error_msg)
            raise SpeechRecognitionError(error_msg)
    
    def get_available_methods(self) -> Dict[SpeechRecognitionMethod, bool]:
        """è·å–å¯ç”¨çš„è¯­éŸ³è¯†åˆ«æ–¹æ³•"""
        return self.available_methods.copy()
    
    def get_supported_languages(self) -> List[LanguageCode]:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return list(LanguageCode)
    
    def get_whisper_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„Whisperæ¨¡å‹åˆ—è¡¨"""
        return ["tiny", "base", "small", "medium", "large"]


def generate_subtitle_for_video(video_path: Path, output_path: Optional[Path] = None, 
                               method: str = "auto", language: str = "auto", 
                               model: str = "base", output_format: str = "srt",
                               enable_fallback: bool = True,
                               openai_api_key: Optional[str] = None,
                               openai_base_url: Optional[str] = None) -> Path:
    """
    ä¸ºè§†é¢‘ç”Ÿæˆå­—å¹•æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„
        method: ç”Ÿæˆæ–¹æ³• ("auto", "bcut_asr", "whisper_local", "openai_api", "azure_speech", "google_speech", "aliyun_speech")
        language: è¯­è¨€ä»£ç 
        model: Whisperæ¨¡å‹å¤§å°ï¼ˆä»…å¯¹whisper_localæœ‰æ•ˆï¼‰
        output_format: è¾“å‡ºæ ¼å¼ ("srt", "vtt", "txt", "json")
        enable_fallback: æ˜¯å¦å¯ç”¨å›é€€æœºåˆ¶
        openai_api_key: OpenAI API Key
        openai_base_url: OpenAI Base URL
        
    Returns:
        ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶è·¯å¾„
        
    Raises:
        SpeechRecognitionError: è¯­éŸ³è¯†åˆ«å¤±è´¥
    """
    # åˆ›å»ºé…ç½®
    config = SpeechRecognitionConfig(
        method=SpeechRecognitionMethod(method) if method != "auto" else SpeechRecognitionMethod.BCUT_ASR,
        language=LanguageCode(language),
        model=model,
        output_format=output_format,
        enable_fallback=enable_fallback,
        openai_api_key=openai_api_key,
        openai_base_url=openai_base_url
    )
    
    recognizer = SpeechRecognizer()
    
    if method == "auto":
        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•
        available_methods = recognizer.get_available_methods()
        
        # æŒ‰ä¼˜å…ˆçº§é€‰æ‹©æ–¹æ³•ï¼ˆbcut-asrä¼˜å…ˆï¼Œå› ä¸ºé€Ÿåº¦æ›´å¿«ï¼‰
        priority_methods = [
            SpeechRecognitionMethod.WHISPER_LOCAL,
            # SpeechRecognitionMethod.BCUT_ASR,
            SpeechRecognitionMethod.OPENAI_API,
            SpeechRecognitionMethod.AZURE_SPEECH,
            SpeechRecognitionMethod.GOOGLE_SPEECH,
            SpeechRecognitionMethod.ALIYUN_SPEECH
        ]
        
        for priority_method in priority_methods:
            if available_methods.get(priority_method, False):
                config.method = priority_method
                break
        else:
            raise SpeechRecognitionError("æ²¡æœ‰å¯ç”¨çš„è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œè¯·å®‰è£…whisperæˆ–é…ç½®APIå¯†é’¥")
    
    return recognizer.generate_subtitle(video_path, output_path, config)


def get_available_speech_recognition_methods() -> Dict[str, bool]:
    """
    è·å–å¯ç”¨çš„è¯­éŸ³è¯†åˆ«æ–¹æ³•
    
    Returns:
        å¯ç”¨æ–¹æ³•å­—å…¸
    """
    recognizer = SpeechRecognizer()
    available_methods = recognizer.get_available_methods()
    
    return {
        method.value: available 
        for method, available in available_methods.items()
    }


def get_supported_languages() -> List[str]:
    """
    è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    
    Returns:
        æ”¯æŒçš„è¯­è¨€ä»£ç åˆ—è¡¨
    """
    return [lang.value for lang in LanguageCode]


def get_whisper_models() -> List[str]:
    """
    è·å–å¯ç”¨çš„Whisperæ¨¡å‹åˆ—è¡¨
    
    Returns:
        Whisperæ¨¡å‹åˆ—è¡¨
    """
    return ["tiny", "base", "small", "medium", "large"]
